"""
ç²¾ç®€ Conic10K æ•°æ®é›†ï¼Œåªä¿ç•™é¢˜ç›®å’Œæ¨ç†è¿‡ç¨‹
ç”¨äºç›´æ¥ä¼ ç»™ Cursor AI Agent åˆ†æå®šç†æ¨¡å¼
"""

import json
import sys
from pathlib import Path


def simplify_dataset(
    input_path: str,
    output_path: str,
    keep_field: str = 'text',
    max_samples: int = None,
    only_with_process: bool = True
):
    """
    ç²¾ç®€æ•°æ®é›†ï¼Œåªä¿ç•™å…³é”®å­—æ®µ
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„ (å¦‚ Conic10K/conic10k/train.json)
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¦‚ scripts/data/simplified_train.json)
        keep_field: ä¿ç•™çš„é¢˜ç›®å­—æ®µï¼Œ'text' æˆ– 'fact_expressions'
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
        only_with_process: æ˜¯å¦åªä¿ç•™æœ‰processçš„æ ·æœ¬
    """
    
    print("="*80)
    print("ç²¾ç®€ Conic10K æ•°æ®é›†")
    print("="*80)
    
    # è¯»å–æ•°æ®
    print(f"\n1. è¯»å–æ•°æ®: {input_path}")
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"   æ€»æ ·æœ¬æ•°: {len(data)}")
    
    # ç­›é€‰æ ·æœ¬
    if only_with_process:
        filtered_data = [
            item for item in data 
            if item.get('process', '').strip()
        ]
        print(f"   æœ‰processçš„æ ·æœ¬: {len(filtered_data)}")
    else:
        filtered_data = data
    
    # é™åˆ¶æ ·æœ¬æ•°
    if max_samples and len(filtered_data) > max_samples:
        filtered_data = filtered_data[:max_samples]
        print(f"   æˆªå–å‰ {max_samples} ä¸ªæ ·æœ¬")
    
    # ç²¾ç®€æ•°æ®ç»“æ„
    print(f"\n2. ç²¾ç®€æ•°æ®ï¼ˆä¿ç•™å­—æ®µ: id, {keep_field}, processï¼‰")
    simplified_data = []
    
    total_chars_before = 0
    total_chars_after = 0
    
    for i, item in enumerate(filtered_data, 1):
        # è®¡ç®—åŸå§‹å¤§å°
        original_str = json.dumps(item, ensure_ascii=False)
        total_chars_before += len(original_str)
        
        # åˆ›å»ºç²¾ç®€ç‰ˆæœ¬
        simplified_item = {
            'id': i,
            'question': item.get(keep_field, ''),
            'process': item.get('process', '')
        }
        
        # è®¡ç®—ç²¾ç®€åå¤§å°
        simplified_str = json.dumps(simplified_item, ensure_ascii=False)
        total_chars_after += len(simplified_str)
        
        simplified_data.append(simplified_item)
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\n3. ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   ç²¾ç®€åæ ·æœ¬æ•°: {len(simplified_data)}")
    print(f"   åŸå§‹æ€»å­—ç¬¦æ•°: {total_chars_before:,}")
    print(f"   ç²¾ç®€åå­—ç¬¦æ•°: {total_chars_after:,}")
    print(f"   å‹ç¼©ç‡: {(1 - total_chars_after/total_chars_before)*100:.1f}%")
    print(f"   ä¼°ç®—tokens: {total_chars_after // 2.5:,.0f}")
    
    # è¯„ä¼°æ˜¯å¦é€‚åˆ200Kä¸Šä¸‹æ–‡
    estimated_tokens = total_chars_after // 2.5
    if estimated_tokens <= 180000:  # ç•™20Kç»™promptå’Œè¾“å‡º
        print(f"   âœ… é€‚åˆ200Kä¸Šä¸‹æ–‡ï¼ˆå‰©ä½™çº¦ {200000 - estimated_tokens:,.0f} tokensï¼‰")
    else:
        print(f"   âš ï¸  è¶…å‡º200Kä¸Šä¸‹æ–‡ï¼éœ€è¦å‡å°‘åˆ°çº¦ {int(len(simplified_data) * 180000 / estimated_tokens)} ä¸ªæ ·æœ¬")
    
    # ä¿å­˜ç»“æœ
    print(f"\n4. ä¿å­˜åˆ°: {output_path}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(simplified_data, f, indent=2, ensure_ascii=False)
    
    print(f"   âœ“ ä¿å­˜æˆåŠŸ")
    
    # ç”Ÿæˆåˆ†æpromptå»ºè®®
    print("\n" + "="*80)
    print("ğŸ’¡ ä½¿ç”¨å»ºè®®")
    print("="*80)
    print(f"""
1. åœ¨ Cursor ä¸­æ‰“å¼€: {output_path}

2. ä½¿ç”¨ä»¥ä¸‹ prompt è®© AI Agent åˆ†æ:

---
è¯·åˆ†æè¿™ä¸ªæ•°å­¦é—®é¢˜æ•°æ®é›†ï¼Œè¯†åˆ«å‡ºæœ€å¸¸ç”¨çš„æ•°å­¦å®šç†å’Œä»£æ•°æ“ä½œã€‚

æ•°æ®æ ¼å¼ï¼š
- question: é—®é¢˜æè¿°
- process: æ±‚è§£è¿‡ç¨‹

è¯·è¾“å‡ºï¼š
1. æœ€å¸¸ç”¨çš„æ•°å­¦å®šç†ï¼ˆTop 30ï¼‰ï¼ŒæŒ‰é¢‘ç‡æ’åº
   æ ¼å¼ï¼šå®šç†åç§° | ä½¿ç”¨æ¬¡æ•° | å®šä¹‰/å…¬å¼
   
2. æœ€å¸¸ç”¨çš„ä»£æ•°æ“ä½œï¼ˆTop 20ï¼‰ï¼ŒæŒ‰é¢‘ç‡æ’åº
   æ ¼å¼ï¼šæ“ä½œåç§° | ä½¿ç”¨æ¬¡æ•° | è¯´æ˜

3. å®šç†åˆ†ç±»ï¼ˆæŒ‰åœ†é”¥æ›²çº¿ç±»å‹ï¼‰ï¼š
   - æ¤­åœ†ç›¸å…³å®šç†
   - åŒæ›²çº¿ç›¸å…³å®šç†
   - æŠ›ç‰©çº¿ç›¸å…³å®šç†
   - é€šç”¨å®šç†

è¯·è¯¦ç»†åˆ†æï¼Œç»™å‡ºå…·ä½“ä¾‹å­ã€‚
---

3. æˆ–è€…å°†æ•°æ®åˆ†æˆå¤šæ‰¹æ¬¡åˆ†æï¼š
   - ç¬¬1æ‰¹: æ ·æœ¬ 1-1000
   - ç¬¬2æ‰¹: æ ·æœ¬ 1001-2000
   - ...
""")
    
    return simplified_data


def create_batches(
    input_path: str,
    output_dir: str,
    batch_size: int = 1000,
    keep_field: str = 'text'
):
    """
    åˆ›å»ºå¤šä¸ªæ‰¹æ¬¡æ–‡ä»¶ï¼Œæ¯ä¸ªæ‰¹æ¬¡é€‚åˆ200Kä¸Šä¸‹æ–‡
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        batch_size: æ¯æ‰¹æ¬¡æ ·æœ¬æ•°
        keep_field: ä¿ç•™çš„é¢˜ç›®å­—æ®µ
    """
    
    print("="*80)
    print("åˆ›å»ºæ‰¹æ¬¡æ–‡ä»¶")
    print("="*80)
    
    # è¯»å–æ•°æ®
    print(f"\nè¯»å–æ•°æ®: {input_path}")
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ç­›é€‰æœ‰processçš„æ ·æœ¬
    filtered_data = [
        item for item in data 
        if item.get('process', '').strip()
    ]
    
    print(f"æ€»æ ·æœ¬æ•°: {len(data)}")
    print(f"æœ‰processçš„æ ·æœ¬: {len(filtered_data)}")
    
    # è®¡ç®—æ‰¹æ¬¡æ•°
    num_batches = (len(filtered_data) + batch_size - 1) // batch_size
    print(f"æ‰¹æ¬¡æ•°: {num_batches} (æ¯æ‰¹æ¬¡ {batch_size} ä¸ªæ ·æœ¬)")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆæ‰¹æ¬¡æ–‡ä»¶
    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(filtered_data))
        
        batch_data = filtered_data[start_idx:end_idx]
        
        # ç²¾ç®€æ•°æ®
        simplified_batch = []
        for i, item in enumerate(batch_data, start_idx + 1):
            simplified_batch.append({
                'id': i,
                'question': item.get(keep_field, ''),
                'process': item.get('process', '')
            })
        
        # ä¿å­˜æ‰¹æ¬¡æ–‡ä»¶
        batch_file = output_path / f'batch_{batch_idx+1:02d}_samples_{start_idx+1}-{end_idx}.json'
        with open(batch_file, 'w', encoding='utf-8') as f:
            json.dump(simplified_batch, f, indent=2, ensure_ascii=False)
        
        # ç»Ÿè®¡
        batch_str = json.dumps(simplified_batch, ensure_ascii=False)
        estimated_tokens = len(batch_str) // 2.5
        
        print(f"  æ‰¹æ¬¡ {batch_idx+1}: {batch_file.name}")
        print(f"    æ ·æœ¬: {start_idx+1}-{end_idx} ({len(simplified_batch)}ä¸ª)")
        print(f"    ä¼°ç®—tokens: {estimated_tokens:,.0f}")
    
    print(f"\nâœ“ æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    
    # ========== é…ç½® ==========
    INPUT_PATH = 'Conic10K/conic10k/train.json'
    
    # é€‰é¡¹1: ç”Ÿæˆå•ä¸ªç²¾ç®€æ–‡ä»¶ï¼ˆæ¨èç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    OUTPUT_SINGLE = 'scripts/data/simplified_train.json'
    MAX_SAMPLES = 1000  # çº¦1000ä¸ªæ ·æœ¬é€‚åˆ200Kä¸Šä¸‹æ–‡ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
    
    # é€‰é¡¹2: ç”Ÿæˆå¤šä¸ªæ‰¹æ¬¡æ–‡ä»¶ï¼ˆæ¨èç”¨äºå®Œæ•´åˆ†æï¼‰
    OUTPUT_BATCH_DIR = 'scripts/data/batches'
    BATCH_SIZE = 1000
    
    # ========== é€‰æ‹©æ‰§è¡Œæ¨¡å¼ ==========
    print("è¯·é€‰æ‹©æ¨¡å¼ï¼š")
    print("1. ç”Ÿæˆå•ä¸ªç²¾ç®€æ–‡ä»¶ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼Œé»˜è®¤1000ä¸ªæ ·æœ¬ï¼‰")
    print("2. ç”Ÿæˆå¤šä¸ªæ‰¹æ¬¡æ–‡ä»¶ï¼ˆå®Œæ•´åˆ†æï¼Œæ¯æ‰¹æ¬¡1000ä¸ªæ ·æœ¬ï¼‰")
    print("3. ç”Ÿæˆå•ä¸ªç²¾ç®€æ–‡ä»¶ï¼ˆå…¨éƒ¨æ ·æœ¬ï¼‰")
    
    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ [1/2/3, é»˜è®¤1]: ").strip() or "1"
    
    if choice == "1":
        print("\né€‰æ‹©: ç”Ÿæˆå•ä¸ªç²¾ç®€æ–‡ä»¶ï¼ˆ1000ä¸ªæ ·æœ¬ï¼‰\n")
        simplify_dataset(
            input_path=INPUT_PATH,
            output_path=OUTPUT_SINGLE,
            keep_field='text',
            max_samples=MAX_SAMPLES,
            only_with_process=True
        )
    
    elif choice == "2":
        print("\né€‰æ‹©: ç”Ÿæˆå¤šä¸ªæ‰¹æ¬¡æ–‡ä»¶\n")
        create_batches(
            input_path=INPUT_PATH,
            output_dir=OUTPUT_BATCH_DIR,
            batch_size=BATCH_SIZE,
            keep_field='text'
        )
    
    elif choice == "3":
        print("\né€‰æ‹©: ç”Ÿæˆå•ä¸ªç²¾ç®€æ–‡ä»¶ï¼ˆå…¨éƒ¨æ ·æœ¬ï¼‰\n")
        simplify_dataset(
            input_path=INPUT_PATH,
            output_path=OUTPUT_SINGLE,
            keep_field='text',
            max_samples=None,
            only_with_process=True
        )
    
    else:
        print("æ— æ•ˆé€‰é¡¹ï¼")
        return
    
    print("\n" + "="*80)
    print("âœ“ å®Œæˆï¼")
    print("="*80)


if __name__ == '__main__':
    main()

