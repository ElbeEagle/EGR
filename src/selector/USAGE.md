# æ¨¡å‹é€‰æ‹©å™¨ä½¿ç”¨æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
cd /Users/ebeleagel/Documents/GitHub/EGR
pip install torch numpy
```

### 2. è¿è¡Œæµ‹è¯•ï¼ˆæ¨èå…ˆè¿è¡Œï¼‰

```bash
python scripts/selector/test.py
```

è¿™ä¼šæµ‹è¯•ï¼š
- âœ… æ•°æ®åŠ è½½
- âœ… æ¨¡å‹åˆ›å»º
- âœ… å‰å‘ä¼ æ’­
- âœ… å°è§„æ¨¡è®­ç»ƒï¼ˆ5 epochsï¼‰

### 3. å®Œæ•´è®­ç»ƒ

```bash
python scripts/selector/train.py
```

è®­ç»ƒé…ç½®ï¼š
- æ•°æ®: `data/train_state_model.json` (135æ ·æœ¬)
- Epochs: 100
- Batch size: 16
- å­¦ä¹ ç‡: 0.001
- Early stopping: 20 epochs patience

é¢„è®¡ç”¨æ—¶ï¼š1-2åˆ†é’Ÿï¼ˆCPUï¼‰

### 4. æŸ¥çœ‹ç»“æœ

è®­ç»ƒå®Œæˆåï¼š
```bash
# æ¨¡å‹æƒé‡
ls -lh checkpoints/model_selector.pth

# è®­ç»ƒæŠ¥å‘Š
cat outputs/selector/training_metrics.json
```

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: è®­ç»ƒæ¨¡å‹

```python
from src.selector import train_model

# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
model, history = train_model()

# æŸ¥çœ‹è®­ç»ƒå†å²
print(f"æœ€ä½³éªŒè¯Top-1å‡†ç¡®ç‡: {max(history['val_acc']):.3f}")
print(f"æœ€ä½³éªŒè¯Top-3å‡†ç¡®ç‡: {max(history['val_top3_acc']):.3f}")
print(f"æœ€ä½³éªŒè¯Top-5å‡†ç¡®ç‡: {max(history['val_top5_acc']):.3f}")
```

### ç¤ºä¾‹2: åŠ è½½æ¨¡å‹å¹¶æ¨ç†

```python
import torch
from src.selector import MaxEntropyClassifier

# åˆ›å»ºæ¨¡å‹
model = MaxEntropyClassifier()

# åŠ è½½æƒé‡
checkpoint = torch.load('checkpoints/model_selector.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# å‡†å¤‡è¾“å…¥ï¼ˆ28ç»´çŠ¶æ€å‘é‡ï¼‰
state_vector = torch.randn(28)

# æ¨ç†
probs, best_model, entropy = model.predict(state_vector)

print(f"é¢„æµ‹çš„æœ€ä¼˜æ¨¡å‹ID: {best_model}")
print(f"è¯¥æ¨¡å‹çš„æ¦‚ç‡: {probs[best_model]:.4f}")
print(f"é¢„æµ‹ç†µ H(Y|X): {entropy:.4f}")
```

### ç¤ºä¾‹3: è·å–Top-Kå€™é€‰

```python
import torch
from src.selector import MaxEntropyClassifier

# åŠ è½½æ¨¡å‹
model = MaxEntropyClassifier()
checkpoint = torch.load('checkpoints/model_selector.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# è¾“å…¥çŠ¶æ€
state_vector = torch.randn(28)

# è·å–Top-5å€™é€‰
top_k_probs, top_k_ids = model.get_top_k(state_vector, k=5)

print("Top-5å€™é€‰æ¨¡å‹:")
for i, (prob, model_id) in enumerate(zip(top_k_probs, top_k_ids)):
    print(f"  Rank {i+1}: Model {model_id.item()} (æ¦‚ç‡={prob.item():.4f})")
```

### ç¤ºä¾‹4: è‡ªå®šä¹‰è®­ç»ƒé…ç½®

```python
from src.selector import train_model

model, history = train_model(
    data_path='data/train_state_model.json',
    num_epochs=50,              # å‡å°‘è½®æ•°
    batch_size=8,               # æ›´å°çš„batch
    learning_rate=0.0005,       # æ›´å°çš„å­¦ä¹ ç‡
    use_class_weights=True,     # ä½¿ç”¨ç±»åˆ«æƒé‡
    patience=15,                # æ›´æ—©çš„early stopping
    save_path='checkpoints/my_model.pth',
    device='cpu'                # æˆ– 'cuda'
)
```

### ç¤ºä¾‹5: è¯„ä¼°å·²è®­ç»ƒçš„æ¨¡å‹

```python
from src.selector import evaluate_model

metrics = evaluate_model(
    model_path='checkpoints/model_selector.pth',
    data_path='data/train_state_model.json'
)

print("è¯„ä¼°ç»“æœ:")
print(f"  Loss: {metrics['loss']:.4f}")
print(f"  Top-1 Acc: {metrics['top1_acc']:.3f}")
print(f"  Top-3 Acc: {metrics['top3_acc']:.3f}")
print(f"  Top-5 Acc: {metrics['top5_acc']:.3f}")
```

### ç¤ºä¾‹6: æ‰¹é‡æ¨ç†

```python
import torch
from src.selector import MaxEntropyClassifier

# åŠ è½½æ¨¡å‹
model = MaxEntropyClassifier()
checkpoint = torch.load('checkpoints/model_selector.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# æ‰¹é‡è¾“å…¥ï¼ˆä¾‹å¦‚8ä¸ªçŠ¶æ€ï¼‰
batch_states = torch.randn(8, 28)

# æ‰¹é‡æ¨ç†
probs, best_models, avg_entropy = model.predict(batch_states)

print(f"æ‰¹é‡å¤§å°: {batch_states.shape[0]}")
print(f"è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒshape: {probs.shape}")  # [8, 80]
print(f"æœ€ä¼˜æ¨¡å‹IDs: {best_models}")          # [8]
print(f"å¹³å‡é¢„æµ‹ç†µ: {avg_entropy:.4f}")
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### ä½¿ç”¨ç±»åˆ«æƒé‡å¤„ç†ä¸å¹³è¡¡

```python
from src.selector import train_model

# å¼€å¯ç±»åˆ«æƒé‡
model, history = train_model(
    use_class_weights=True  # å¯¹ä½é¢‘æ¨¡å‹å¢åŠ æƒé‡
)
```

### æ‰‹åŠ¨æ§åˆ¶è®­ç»ƒå¾ªç¯

```python
from src.selector import (
    load_training_data,
    prepare_dataloaders,
    MaxEntropyClassifier,
    Trainer
)
import torch

# 1. åŠ è½½æ•°æ®
X, y = load_training_data('data/train_state_model.json')
train_loader, val_loader = prepare_dataloaders(X, y, batch_size=16)

# 2. åˆ›å»ºæ¨¡å‹
model = MaxEntropyClassifier()

# 3. åˆ›å»ºè®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    device='cpu',
    learning_rate=0.001,
    use_class_weights=False
)

# 4. è®­ç»ƒ
history = trainer.fit(
    train_loader=train_loader,
    val_loader=val_loader,
    num_epochs=100,
    patience=20,
    save_path='checkpoints/model_selector.pth'
)

# 5. æŸ¥çœ‹å†å²
import matplotlib.pyplot as plt
plt.plot(history['train_loss'], label='Train Loss')
plt.plot(history['val_loss'], label='Val Loss')
plt.legend()
plt.savefig('loss_curve.png')
```

### åœ¨æ¨ç†å¼•æ“ä¸­ä½¿ç”¨ï¼ˆModule 4é›†æˆï¼‰

```python
import torch
from src.selector import MaxEntropyClassifier

class ReasoningEngine:
    def __init__(self):
        # åŠ è½½æ¨¡å‹é€‰æ‹©å™¨
        self.classifier = MaxEntropyClassifier()
        checkpoint = torch.load('checkpoints/model_selector.pth')
        self.classifier.load_state_dict(checkpoint['model_state_dict'])
        self.classifier.eval()
    
    def select_model(self, symbolic_state, abstract_state):
        """
        é€‰æ‹©ä¸‹ä¸€ä¸ªè¦åº”ç”¨çš„æ¨¡å‹
        
        ç»“åˆç¥ç»ç½‘ç»œé¢„æµ‹å’Œè§„åˆ™çº¦æŸ
        """
        # 1. ç¥ç»ç½‘ç»œé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        state_vector = torch.tensor(
            abstract_state.to_vector(), 
            dtype=torch.float32
        )
        probs, _, H_Y_X = self.classifier.predict(state_vector)
        
        # 2. è§„åˆ™ç­›é€‰å¯åº”ç”¨çš„æ¨¡å‹
        valid_models = []
        for model_id in range(80):
            model = self.theorem_library.get_model(model_id)
            if model and model.can_apply(symbolic_state):
                valid_models.append(model_id)
        
        # 3. Maskä¸å¯ç”¨æ¨¡å‹ï¼Œé‡æ–°å½’ä¸€åŒ–
        valid_tensor = torch.tensor(valid_models)
        masked_probs = torch.zeros(80)
        masked_probs[valid_tensor] = probs[valid_tensor]
        
        if masked_probs.sum() > 0:
            masked_probs = masked_probs / masked_probs.sum()
        else:
            # å¦‚æœæ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œè¿”å›None
            return None
        
        # 4. é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„æ¨¡å‹
        best_model = masked_probs.argmax().item()
        
        return best_model, masked_probs[best_model].item(), H_Y_X
```

## ğŸ“Š é¢„æœŸæ€§èƒ½

### å½“å‰æ•°æ®è§„æ¨¡ï¼ˆ135æ ·æœ¬ï¼‰

```
è®­ç»ƒé›† (108æ ·æœ¬):
  - Top-1: ~25-35%
  - Top-3: ~45-55%
  - Top-5: ~55-65%

éªŒè¯é›† (27æ ·æœ¬):
  - Top-1: ~20-30%
  - Top-3: ~40-50%
  - Top-5: ~50-60%
```

### ä¸ºä»€ä¹ˆå‡†ç¡®ç‡ä¸é«˜ï¼Ÿ

1. **æ•°æ®é‡å°**: 135æ ·æœ¬ vs 80ç±»åˆ«
2. **ç±»åˆ«ä¸å¹³è¡¡**: 29ä¸ªæ¨¡å‹æœ‰æ•°æ®ï¼Œ51ä¸ªæ¨¡å‹æ— æ•°æ®
3. **ä»»åŠ¡éš¾åº¦**: å¤šç±»åˆ«åˆ†ç±»æœ¬èº«å°±æœ‰æŒ‘æˆ˜

### å®é™…ä»·å€¼

å³ä½¿Top-1å‡†ç¡®ç‡è¾ƒä½ï¼Œä»ç„¶éå¸¸æœ‰ç”¨ï¼š

1. **ç¼©å°æœç´¢ç©ºé—´**:
   - Top-5å‡†ç¡®ç‡~50% â†’ åªéœ€æœç´¢5ä¸ªå€™é€‰ï¼ˆ6.25%æœç´¢ç©ºé—´ï¼‰
   - æ¯”éšæœºé€‰æ‹©ï¼ˆ1.25%ï¼‰æå‡4å€

2. **æ¦‚ç‡å…ˆéªŒ**:
   - æä¾›80ä¸ªæ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒ
   - ç”¨äºä¿¡æ¯å¢ç›Šè®¡ç®—

3. **ä¸è§„åˆ™ç»“åˆ**:
   - ç¥ç»ç½‘ç»œ + can_applyè§„åˆ™
   - å‡†ç¡®ç‡ä¼šæ˜¾è‘—æå‡

4. **ä¸ç¡®å®šæ€§åº¦é‡**:
   - H(Y|X)é‡åŒ–é¢„æµ‹ç½®ä¿¡åº¦
   - ç”¨äºä¸‰å±‚ç†µæ¶æ„çš„Layer 3

## â“ å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒéœ€è¦GPUå—ï¼Ÿ

**A**: ä¸éœ€è¦ã€‚æ•°æ®é‡å°ï¼ˆ135æ ·æœ¬ï¼‰ï¼ŒCPUè®­ç»ƒ1-2åˆ†é’Ÿå³å¯ã€‚å¦‚æœæœ‰GPUï¼Œè®¾ç½®`device='cuda'`å¯ä»¥æ›´å¿«ã€‚

### Q2: å¦‚ä½•å¢åŠ è®­ç»ƒæ•°æ®ï¼Ÿ

**A**: 
1. æ ‡æ³¨æ›´å¤šConic10Kæ ·æœ¬çš„æ¨¡å‹åºåˆ—
2. æ›´æ–° `data/train_state_model.json`
3. é‡æ–°è¿è¡Œ `python scripts/selector/train.py`

### Q3: æ¨¡å‹æ–‡ä»¶å¤šå¤§ï¼Ÿ

**A**: ~60KBï¼ˆå‚æ•°é‡~15,000ï¼Œfp32å­˜å‚¨ï¼‰

### Q4: å¯ä»¥å¹¶è¡Œè®­ç»ƒå¤šä¸ªæ¨¡å‹å—ï¼Ÿ

**A**: å¯ä»¥ã€‚ä½¿ç”¨ä¸åŒçš„`save_path`ä¿å­˜ä¸åŒé…ç½®çš„æ¨¡å‹ï¼Œç„¶åå¯¹æ¯”æ€§èƒ½ã€‚

### Q5: è®­ç»ƒä¸­æ–­äº†æ€ä¹ˆåŠï¼Ÿ

**A**: æœ€ä½³æ¨¡å‹ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜åˆ°`checkpoints/model_selector.pth`ã€‚é‡æ–°è¿è¡Œä¼šä»å¤´å¼€å§‹ï¼Œä½†å·²ä¿å­˜çš„æœ€ä½³æ¨¡å‹ä¸ä¼šä¸¢å¤±ã€‚

### Q6: å¦‚ä½•å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Ÿ

**A**: è®­ç»ƒå†å²ä¿å­˜åœ¨è¿”å›çš„`history`å­—å…¸ä¸­ï¼Œå¯ä»¥ç”¨matplotlibç»˜åˆ¶ï¼š

```python
import matplotlib.pyplot as plt

model, history = train_model()

plt.figure(figsize=(12, 4))

# æŸå¤±æ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(history['train_loss'], label='Train')
plt.plot(history['val_loss'], label='Val')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss Curve')

# å‡†ç¡®ç‡æ›²çº¿
plt.subplot(1, 2, 2)
plt.plot(history['val_acc'], label='Top-1')
plt.plot(history['val_top3_acc'], label='Top-3')
plt.plot(history['val_top5_acc'], label='Top-5')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Accuracy Curve')

plt.savefig('training_curves.png')
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **æ¨¡å—è¯¦ç»†æ–‡æ¡£**: `src/selector/README.md`
- **å®ŒæˆæŠ¥å‘Š**: `dev_logs/module3_training_completion.md`
- **Module 3è§„æ ¼**: `doc/module3_training_data_constructor.md`

---

**æœ€åæ›´æ–°**: 2026-01-20  
**ç»´æŠ¤è€…**: EGR Team
